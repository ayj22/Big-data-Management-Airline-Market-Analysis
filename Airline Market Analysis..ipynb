{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Airline Market Analysis..**<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">/mnt/flight_data has been unmounted.\n",
       "Out[57]: True</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dbutils.fs.unmount(\"/mnt/flight_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "#CONNECTING WITH AWS IN ORDER TO ACCESS S3 FILES.\n",
    "ACCESS_KEY = \"XXXX\"\n",
    "SECRET_KEY = \"XXXX\"\n",
    "ENCODED_SECRET_KEY = SECRET_KEY.replace(\"/\", \"%2F\")\n",
    "AWS_BUCKET_NAME = \"airplane-data-bigdata\"\n",
    "MOUNT_NAME = \"flight_data\"\n",
    "\n",
    "dbutils.fs.mount(\"s3a://%s:%s@%s\" % (ACCESS_KEY, ENCODED_SECRET_KEY, AWS_BUCKET_NAME), \"/mnt/%s\" % MOUNT_NAME)\n",
    "display(dbutils.fs.ls(\"/mnt/%s\" % MOUNT_NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIST OF FILES \n",
    "display(dbutils.fs.ls(\"/mnt/flight_data/airplane_data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_data = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mnt/flight_data/airplane_data/airOT20*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_data.select('YEAR').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above count queries are taking more time to execute as the whole table is getting traversed,thus there is a need of optimization over these dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_data.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above shown are the default Number of Partitions done by the Spark and there is a need to optimize the partition size for the dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_data.repartition(63).createOrReplaceTempView(\"flight_merged_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching the dataframe to memeory with uodated number of partitions for faster execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.catalog.cacheTable('flight_merged_view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(\"flight_merged_view\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cache creation is done once an action query is triggered on the particular table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_DF = spark.table(\"flight_merged_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As our final dataframe with optimized number of partitions is ready we have saved a parquet file of the data for future use so we can continue with the same file even if the cluster terminates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_merged_DF.write.format('parquet').save('/tmp/flight_merged_parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dbutils.fs.ls(\"/tmp/flight_merged_parquet\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up the Deafult setting of Shuffle Partitions.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.get(\"spark.sql.shuffle.partitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\",63)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we created a merged optimized dataframe,we have saved a \"parquet\" file of the dataframe with the updated number of partitions.Parquet is a columnar file format that provides optimizations to speed up queries and is a far more efficient file format than CSV or JSON, supported by many data processing systems.\n",
    "<br>\n",
    "Later even if the cluster terminates we can use the newly created parquet file with the updated number of partitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis of the Data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count of Flights Against each destination state in the United States from Jan 2000 - Feb 2020.\n",
    "- California and Texas have maximum number of incoming flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FLIGHTS</th><th>DEST_STATE_ABR</th></tr></thead><tbody><tr><td>15425336</td><td>CA</td></tr><tr><td>14449778</td><td>TX</td></tr><tr><td>8989110</td><td>FL</td></tr><tr><td>8486472</td><td>IL</td></tr><tr><td>7884760</td><td>GA</td></tr><tr><td>5902895</td><td>NY</td></tr><tr><td>4689134</td><td>CO</td></tr><tr><td>4140333</td><td>NC</td></tr><tr><td>4111950</td><td>VA</td></tr><tr><td>4082632</td><td>AZ</td></tr><tr><td>3532586</td><td>NV</td></tr><tr><td>3458122</td><td>MI</td></tr><tr><td>3270881</td><td>PA</td></tr><tr><td>2819855</td><td>MN</td></tr><tr><td>2765681</td><td>MO</td></tr><tr><td>2636410</td><td>NJ</td></tr><tr><td>2577210</td><td>WA</td></tr><tr><td>2464310</td><td>MA</td></tr><tr><td>2425557</td><td>TN</td></tr><tr><td>2413319</td><td>UT</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query1 = spark.sql(\"SELECT COUNT(*) AS FLIGHTS, DEST_STATE_ABR FROM flight_merged_view GROUP BY DEST_STATE_ABR ORDER BY FLIGHTS DESC LIMIT 20\")\n",
    "display(query1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the possible causes of delays in the Flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FACTOR</th><th>FLIGHT_DELAYS</th></tr></thead><tbody><tr><td>LATE_AIRCRAFT_DELAY</td><td>4.65640739E8</td></tr><tr><td>CARRIER_DELAY</td><td>3.58894745E8</td></tr><tr><td>NAS_DELAY</td><td>3.27300129E8</td></tr><tr><td>WEATHER_DELAY</td><td>6.2387678E7</td></tr><tr><td>SECURITY_DELAY</td><td>1942565.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query3 = spark.sql(\"SELECT 'CARRIER_DELAY' AS FACTOR, (SELECT SUM(CARRIER_DELAY) FROM flight_merged_view) AS FLIGHT_DELAYS UNION SELECT 'WEATHER_DELAY' AS FACTOR, (SELECT SUM(WEATHER_DELAY) FROM flight_merged_view) AS FLIGHT_DELAYS UNION SELECT 'NAS_DELAY' AS FACTOR, (SELECT SUM(NAS_DELAY) FROM flight_merged_view) AS FLIGHT_DELAYS UNION SELECT 'SECURITY_DELAY' AS FACTOR, (SELECT SUM(SECURITY_DELAY) FROM flight_merged_view) AS FLIGHT_DELAYS UNION SELECT 'LATE_AIRCRAFT_DELAY' AS FACTOR, (SELECT SUM(LATE_AIRCRAFT_DELAY) FROM flight_merged_view) AS FLIGHT_DELAYS ORDER BY FLIGHT_DELAYS DESC\")\n",
    "display(query3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analysing the count of cancelled flights against the days of week.**<br>\n",
    "\n",
    "- Friday has comparatively less number of cancelled flights compared to other days of week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>WEEKDAY</th><th>DAY_OF_WEEK</th><th>FLIGHTS_CANCELLED</th></tr></thead><tbody><tr><td>MON</td><td>2</td><td>378592.0</td></tr><tr><td>TUE</td><td>3</td><td>376648.0</td></tr><tr><td>WED</td><td>4</td><td>363834.0</td></tr><tr><td>SUN</td><td>1</td><td>351376.0</td></tr><tr><td>THU</td><td>5</td><td>339440.0</td></tr><tr><td>SAT</td><td>7</td><td>288615.0</td></tr><tr><td>FRI</td><td>6</td><td>236335.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def intToDay(dayOfWeek):\n",
    "  switcher = {\n",
    "    1: 'SUN',\n",
    "    2: 'MON',\n",
    "    3: 'TUE',\n",
    "    4: 'WED',\n",
    "    5: 'THU',\n",
    "    6: 'FRI',\n",
    "    7: 'SAT'\n",
    "  }\n",
    "  return switcher.get(int(dayOfWeek), 'Invalid DAY_OF_WEEK')\n",
    "\n",
    "spark.udf.register(\"intToDay\", intToDay)\n",
    "query4 = spark.sql(\"SELECT intToDay(DAY_OF_WEEK) AS WEEKDAY, DAY_OF_WEEK, SUM(CANCELLED) AS FLIGHTS_CANCELLED FROM flight_merged_view GROUP BY DAY_OF_WEEK ORDER BY FLIGHTS_CANCELLED DESC\")\n",
    "display(query4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total Number of FLights against each Month (Jan 2000 - Feb 2020)**<br>\n",
    "\n",
    "- June and July having maximum number of flight travels this should be becuase of the summer breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>FLIGHTS</th><th>MONTH_NAME</th><th>MONTH</th></tr></thead><tbody><tr><td>11937664</td><td>Jun</td><td>6</td></tr><tr><td>11376394</td><td>Jul</td><td>7</td></tr><tr><td>11281939</td><td>Sep</td><td>9</td></tr><tr><td>11135746</td><td>Nov</td><td>11</td></tr><tr><td>11119794</td><td>Jan</td><td>1</td></tr><tr><td>11053120</td><td>Mar</td><td>3</td></tr><tr><td>10952679</td><td>May</td><td>5</td></tr><tr><td>10617396</td><td>Apr</td><td>4</td></tr><tr><td>10311516</td><td>Aug</td><td>8</td></tr><tr><td>9784417</td><td>Oct</td><td>10</td></tr><tr><td>9782928</td><td>Feb</td><td>2</td></tr><tr><td>9573186</td><td>Dec</td><td>12</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def intToMonth(month):\n",
    "  switcher = {\n",
    "    1: 'Jan',\n",
    "    2: 'Feb',\n",
    "    3: 'Mar',\n",
    "    4: 'Apr',\n",
    "    5: 'May',\n",
    "    6: 'Jun',\n",
    "    7: 'Jul',\n",
    "    8: 'Aug',\n",
    "    9: 'Sep',\n",
    "    10: 'Oct',\n",
    "    11: 'Nov',\n",
    "    12: 'Dec'\n",
    "  }\n",
    "  return switcher.get(int(month), 'Invalid MONTH')\n",
    "\n",
    "spark.udf.register(\"intToMonth\", intToMonth)\n",
    "query5 = spark.sql(\"SELECT COUNT(*) AS FLIGHTS, intToMonth(MONTH) AS MONTH_NAME, MONTH FROM flight_merged_view GROUP BY MONTH ORDER BY FLIGHTS DESC\")\n",
    "display(query5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Impact of Global Recession On the Airline Industry..!!\n",
    "\n",
    "<br>\n",
    "When housing prices fell and homeowners began to walk away from their mortgages, the value of mortgage-backed securities held by investment banks declined in 2007â€“2008, causing several to collapse on the economy of the United States in the years of 2008 and ahead....\n",
    "\n",
    "source:wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State wise frequency of the flights journey.**\n",
    "- Consistent fall in the Number of flights Journeys in the big states of US.<br>\n",
    "1.California <br>\n",
    "2.Texas<br>\n",
    "3.Illinois<br>\n",
    "3.Florida<br>\n",
    "4.New York<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#STATEWISE DISTRIBUTION FOR THE IMPACTED YEARS..!\n",
    "display(spark.sql('select DEST_STATE_ABR,count(DEST_STATE_ABR) as Frequency_Flights,year from flight_merged_view where CAST(YEAR AS INT) IN (\"2007\",\"2008\",\"2009\") group by YEAR,DEST_STATE_ABR ORDER BY count(DEST_STATE_ABR)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Month wise analysis of the Global recession period.**<br>\n",
    "- November 2008 and Jan 2009 observed a major fall in the number of flights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql('select distinct CAST(MONTH AS INT),COUNT(CAST(MONTH AS INT)) as NUMBER_OF_FLIGHTS ,CAST(YEAR AS INT) from flight_merged_view WHERE YEAR IN(\"2008\",\"2009\") GROUP BY CAST(MONTH AS INT),CAST(YEAR AS INT) ORDER BY CAST(YEAR AS INT),CAST(MONTH AS INT)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identifying the trends in the Frequency of Flights for the Airline Carriers from 2007-2009**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.sql('select UNIQUE_CARRIER,COUNT(UNIQUE_CARRIER) as FREQUENCY,YEAR from flight_merged_view WHERE CAST(YEAR AS INT) IN (\"2007\",\"2008\",\"2009\") group by UNIQUE_CARRIER,YEAR ORDER BY YEAR,COUNT(UNIQUE_CARRIER) DESC'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fraud Data Analysis over Airline Booking Transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/mnt/flight_data/airplane_data/Fraud_Cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Fraud_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fraud_df.registerTempTable('fraud_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of fraudelent risk in male (i.e., 1) and female (i.e., 2) **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--Comparison of fraudelent risk in male (i.e., 1) and female (i.e., 2) \n",
    "display(spark.sql('select gender, count(*) as count from fraud_data f where fraudRisk=1 group by gender'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparison of cardholders which are fraudelent cases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparison of cardholders which are fraudelent cases\n",
    "display(spark.sql('select count(cardholder) as card, cardholder from fraud_data where fraudRisk=0 group by cardholder'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding states which are more prone to fraudelent activities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding states which are more prone to fraudelent activities\n",
    "display(spark.sql('select state, count(state) as state_count from fraud_data where fraudRisk in (1) group by state order by count(state)'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***********END*********"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "name": "2020-05-09 - S3 Example",
  "notebookId": 1251010126247737
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
